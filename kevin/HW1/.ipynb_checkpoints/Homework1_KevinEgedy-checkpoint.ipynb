{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE P 590: Deep Learning for Embedded Real-time Intelligence \n",
    "\n",
    "\n",
    "## Assignment 1 (Due April 17th 11:59 pm) \n",
    "\n",
    "\n",
    "**Note: You can use the codes and helper functions provided to you in the `Tutorial 1.ipynb` and `utils.py` files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from eep590_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up (25 points) \n",
    "\n",
    "**1) Plot the function $y = f(x) = x^3 - \\frac{1}{x}$ and its tangent line when $x = 1$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,axs = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "x = torch.arange(-5, 5, 0.1, requires_grad=True)\n",
    "u=None\n",
    "y=None\n",
    "y = torch.pow(x,3) - 1/x\n",
    "y.sum().backward()\n",
    "grad = x.grad.detach().numpy()\n",
    "slope = grad[np.where(x.detach().numpy()==1)[0][0]]\n",
    "\n",
    "axs.plot(x.detach().numpy(),y.detach().numpy(), label='function')\n",
    "#axs.plot(x.detach().numpy(),x.grad.detach().numpy(), label='gradient at all points')\n",
    "axs.plot(x.detach().numpy(), x.detach().numpy()*slope-4, label='grad(x=1) = 4x-4')\n",
    "axs.set_ylim(-100,100)\n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Why is the second derivative much more expensive to compute than the first derivative?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second derivative is always more expensive due to the fact that the chain rule is needed to find the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Let $f(x) = \\sin(x)$. Plot $f(x)$ and $\\frac{df(x)}{dx}$, where the latter is computed using the inbuilt autograd feature of pytorch.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection (15 points)\n",
    "\n",
    "The projection of a vector $\\mathbf{x}$ on a convex set $\\mathcal{X}$ is defined as\n",
    "\n",
    "$$\\mathrm{Proj}_\\mathcal{X}(\\mathbf{x}) = \\mathop{\\mathrm{argmin}}_{\\mathbf{x}' \\in \\mathcal{X}} \\|\\mathbf{x} - \\mathbf{x}'\\|,$$\n",
    "\n",
    "which is the closest point in $\\mathcal{X}$ to $\\mathbf{x}$. \n",
    "\n",
    "![Convex Projections.](projections.svg)\n",
    "\n",
    "The mathematical definition of projections may sound a bit abstract. The figure explains it somewhat more clearly. In it we have two convex sets, a circle and a diamond. \n",
    "Points inside both sets (yellow) remain unchanged during projections. \n",
    "Points outside both sets (black) are projected to \n",
    "the points inside the sets (red) that are closet to the original points (black).\n",
    "While for $L_2$ balls this leaves the direction of the vector unchanged, this need not be the case in general, as can be seen in the case of the diamond.\n",
    "\n",
    "\n",
    "One of the uses for convex projections is to compute sparse weight vectors. In this case we project weight vectors onto an $L_1$ ball, which is a generalized version of the diamond case in the figure. Later in this course we will learn why sparsity is a powerful tool in embedded deep learning, and exploit sparsity for efficient real-time inference. \n",
    "\n",
    "**1) Given a convex set $\\mathcal{X}$ and vectors $\\mathbf{x}$ and $\\mathbf{y}$, is the condition $\\|\\mathbf{x} - \\mathbf{y}\\| \\geq \\|\\mathrm{Proj}_\\mathcal{X}(\\mathbf{x}) - \\mathrm{Proj}_\\mathcal{X}(\\mathbf{y})\\|$ always true? Explain your reasoning (we don't expect a formal mathematical proof, an intuitive explanation is sufficient)** \n",
    "\n",
    "Note: here $\\|\\mathbf{x} - \\mathbf{y}\\|$ represents the euclidean distance between $\\mathbf{x}$ and $\\mathbf{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced bit-width weights (10 points)\n",
    "\n",
    "We will learn later in the class that often it is sufficient to express weights of deep learning in reduced 4-bit or 8-bit  integer repsentations, instead of using real numbers (floating points or 32-bit representations). The process of doing so is called **quantization**. This reduces the memory required to store the weights in MCU units by a factor of 4 to 8, with a marginal trade-off for accuracy. \n",
    "\n",
    "**1) Consider the set of all the points that can be represented as $n$-bit integers and let's call the set as $\\mathcal{X}_n$. For example, $\\mathcal{X}_8$ will be a set of integers from $0$ to $255$. Is  $\\mathcal{X}_4$ a convex set? What about $\\mathcal{X}_n$ for a general $n$? Explain your reasoning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression (20 points)\n",
    "\n",
    "**1) Implement the closed-form (analytical) solution for linear regression (refer to lecture slides) and compare it with the parameters obtained using SGD in `tutorial 1.ipynb`. How does this problem and its solution relate to the normal distribution?**  \n",
    "\n",
    "\n",
    "**2) In the linear regression example provided in tutorial 1, what would happen if we were to initialize the weights to zero. Would the algorithm work? Explain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum (30 points)\n",
    "\n",
    "In the class lecture and the tutorial we reviewed stochastic gradient descent, i.e., when performing optimization where only a noisy variant of the gradient is available. For noisy gradients we need to be extra cautious when it comes to choosing the learning rate in the face of noise. If we decrease it too rapidly, convergence stalls. If we are too lenient, we fail to converge to a good enough solution since noise keeps on driving us away from optimality. Here we will explore more effective optimization algorithms, especially for certain types of optimization problems that are common in practice.\n",
    "\n",
    "\n",
    "#### Leaky Averages\n",
    "\n",
    "In the lecture we discussed minibatch SGD as a means for accelerating computation. It also had the nice side-effect that averaging gradients reduced the amount of variance. The minibatch stochastic gradient descent can be calculated by:\n",
    "\n",
    "$$\\mathbf{g}_{t, t-1} = \\partial_{\\mathbf{w}} \\frac{1}{|\\mathcal{B}_t|} \\sum_{i \\in \\mathcal{B}_t} f(\\mathbf{x}_{i}, \\mathbf{w}_{t-1}) = \\frac{1}{|\\mathcal{B}_t|} \\sum_{i \\in \\mathcal{B}_t} \\mathbf{h}_{i, t-1}.\n",
    "$$\n",
    "\n",
    "To keep the notation simple, here we used $\\mathbf{h}_{i, t-1} = \\partial_{\\mathbf{w}} f(\\mathbf{x}_i, \\mathbf{w}_{t-1})$ as the stochastic gradient descent for sample $i$ using the weights updated at time $t-1$.\n",
    "It would be nice if we could benefit from the effect of variance reduction even beyond averaging gradients on a minibatch. One option to accomplish this task is to replace the gradient computation by a \"leaky average\":\n",
    "\n",
    "$$\\mathbf{v}_t = \\beta \\mathbf{v}_{t-1} + \\mathbf{g}_{t, t-1}$$\n",
    "\n",
    "for some $\\beta \\in (0, 1)$. This effectively replaces the instantaneous gradient by one that's been averaged over multiple *past* gradients. $\\mathbf{v}$ is called *momentum*. It accumulates past gradients similar to how a heavy ball rolling down the objective function landscape integrates over past forces. To see what is happening in more detail let us expand $\\mathbf{v}_t$ recursively into\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{v}_t = \\beta^2 \\mathbf{v}_{t-2} + \\beta \\mathbf{g}_{t-1, t-2} + \\mathbf{g}_{t, t-1}\n",
    "= \\ldots, = \\sum_{\\tau = 0}^{t-1} \\beta^{\\tau} \\mathbf{g}_{t-\\tau, t-\\tau-1}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "Large $\\beta$ amounts to a long-range average, whereas small $\\beta$ amounts to only a slight correction relative to a gradient method. The new gradient replacement no longer points into the direction of steepest descent on a particular instance any longer but rather in the direction of a weighted average of past gradients. This allows us to realize most of the benefits of averaging over a batch without the cost of actually computing the gradients on it. We will revisit this averaging procedure in more detail later.\n",
    "\n",
    "The above reasoning formed the basis for what is now known as *accelerated* gradient methods, such as gradients with momentum. They enjoy the additional benefit of being much more effective in cases where the optimization problem is ill-conditioned (i.e., where there are some directions where progress is much slower than in others, resembling a narrow canyon). Furthermore, they allow us to average over subsequent gradients to obtain more stable directions of descent. Indeed, the aspect of acceleration even for noise-free convex problems is one of the key reasons why momentum works and why it works so well.\n",
    "\n",
    "#### An Ill-conditioned Example\n",
    "\n",
    "To get a better understanding of the geometric properties of the momentum method we revisit gradient descent, albeit with a significantly less pleasant objective function. We distort an ellipsoid function by stretching it out in the $x_1$ direction via\n",
    "\n",
    "$$f(\\mathbf{x}) = 0.1 x_1^2 + 2 x_2^2.$$\n",
    "\n",
    "$f$ has its minimum at $(0, 0)$. This function is *very* flat in the direction of $x_1$. Let us see what happens when we perform gradient descent as before on this new function. We pick a learning rate of $0.4$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.4\n",
    "def f_2d(x1, x2):\n",
    "    return 0.1 * x1 ** 2 + 2 * x2 ** 2\n",
    "def gd_2d(x1, x2, s1, s2):\n",
    "    return (x1 - eta * 0.2 * x1, x2 - eta * 4 * x2, 0, 0)\n",
    "\n",
    "show_trace_2d(f_2d, train_2d(gd_2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By construction, the gradient in the $x_2$ direction is *much* higher and changes much more rapidly than in the horizontal $x_1$ direction. Thus we are stuck between two undesirable choices: if we pick a small learning rate we ensure that the solution does not diverge in the $x_2$ direction but we are saddled with slow convergence in the $x_1$ direction. Conversely, with a large learning rate we progress rapidly in the $x_1$ direction but diverge in $x_2$. The example below illustrates what happens even after a slight increase in learning rate from $0.4$ to $0.6$. Convergence in the $x_1$ direction improves but the overall solution quality is much worse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.6\n",
    "show_trace_2d(f_2d, train_2d(gd_2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Momentum Method\n",
    "\n",
    "The momentum method allows us to solve the gradient descent problem described\n",
    "above. Looking at the optimization trace above we might intuit that averaging gradients over the past would work well. After all, in the $x_1$ direction this will aggregate well-aligned gradients, thus increasing the distance we cover with every step. Conversely, in the $x_2$ direction where gradients oscillate, an aggregate gradient will reduce step size due to oscillations that cancel each other out.\n",
    "Using $\\mathbf{v}_t$ instead of the gradient $\\mathbf{g}_t$ yields the following update equations:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{v}_t &\\leftarrow \\beta \\mathbf{v}_{t-1} + \\mathbf{g}_{t, t-1}, \\\\\n",
    "\\mathbf{x}_t &\\leftarrow \\mathbf{x}_{t-1} - \\eta_t \\mathbf{v}_t.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that for $\\beta = 0$ we recover regular gradient descent. Let us have a quick look at how the algorithm behaves in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_2d(x1, x2, v1, v2):\n",
    "    v1 = beta * v1 + 0.2 * x1\n",
    "    v2 = beta * v2 + 4 * x2\n",
    "    return x1 - eta * v1, x2 - eta * v2, v1, v2\n",
    "\n",
    "eta, beta = 0.6, 0.5\n",
    "show_trace_2d(f_2d, train_2d(momentum_2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, even with the same learning rate that we used before, momentum still converges well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can combine momentum with stochastic gradient descent and in particular, minibatch stochastic gradient descent. Let us see how momentum works in practice, i.e., when used within the context of a proper optimizer. For this we need a somewhat more scalable implementation.\n",
    "\n",
    "### 1) Implementation (complete the modules so that this works end-to-end)\n",
    "\n",
    "Compared with (minibatch) stochastic gradient descent the momentum method needs to maintain a set of  auxiliary variables, i.e., velocity. It has the same shape as the gradients (and variables of the optimization problem). In the implementation below we call these variables `states`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_momentum_states(feature_dim):\n",
    "    '''Complete this module and return _momentum_states''''\n",
    "    return _momentum_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_momentum(params, states, hyperparams):\n",
    "    '''Complete this module. You can refer to sgd() module in the tutorial for a reference '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Dataset\n",
    "\n",
    "In the following we use a dataset developed by NASA to test the wing [noise from different aircraft](https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise) For convenience we only use the first $1,500$ examples. The data is whitened for preprocessing, i.e., we remove the mean and rescale the variance to $1$ per coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size=10, n=1500):\n",
    "    data = np.genfromtxt('airfoil_self_noise.dat',\n",
    "                         dtype=np.float32, delimiter='\\t')\n",
    "    data = torch.from_numpy((data - data.mean(axis=0)) / data.std(axis=0))\n",
    "    data_iter = load_array((data[:n, :-1], data[:n, -1]),\n",
    "                               batch_size, is_train=True)\n",
    "    return data_iter, data.shape[1]-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement a generic training function to facilitate the use of the other optimization algorithms introduced later in this chapter. It initializes a linear regression model and can be used to train the model with minibatch stochastic gradient descent and other algorithms introduced subsequently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train(trainer_fn, states, hyperparams, data_iter,\n",
    "               feature_dim, num_epochs=2):\n",
    "    # Initialization\n",
    "    w = torch.normal(mean=0.0, std=0.01, size=(feature_dim, 1),\n",
    "                     requires_grad=True)\n",
    "    b = torch.zeros((1), requires_grad=True)\n",
    "    net, loss = lambda X: linreg(X, w, b), squared_loss\n",
    "    # Train\n",
    "    animator = Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[0, num_epochs], ylim=[0.22, 0.35])\n",
    "    n, timer = 0, Timer()\n",
    "    for _ in range(num_epochs):\n",
    "        for X, y in data_iter:\n",
    "            l = loss(net(X), y).mean()\n",
    "            l.backward()\n",
    "            trainer_fn([w, b], states, hyperparams)\n",
    "            n += X.shape[0]\n",
    "            if n % 200 == 0:\n",
    "                timer.stop()\n",
    "                animator.add(n/X.shape[0]/len(data_iter),\n",
    "                             (evaluate_loss(net, data_iter, loss),))\n",
    "                timer.start()\n",
    "    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.avg():.3f} sec/epoch')\n",
    "    return timer.cumsum(), animator.Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_momentum(lr, momentum, num_epochs=2):\n",
    "    _train(sgd_momentum, init_momentum_states(feature_dim),\n",
    "                   {'lr': lr, 'momentum': momentum}, data_iter,\n",
    "                   feature_dim, num_epochs)\n",
    "\n",
    "data_iter, feature_dim = get_data(batch_size=10)\n",
    "train_momentum(0.02, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_momentum(0.01, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_momentum(0.005, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Momentum for Scalar Functions\n",
    "\n",
    "Now, let us see what happens when we minimize the function $f(x) = \\frac{\\lambda}{2} x^2$. For gradient descent we have\n",
    "\n",
    "$$x_{t+1} = x_t - \\eta \\lambda x_t = (1 - \\eta \\lambda) x_t.$$\n",
    "\n",
    "Whenever $|1 - \\eta \\lambda| < 1$ this optimization converges at an exponential rate since after $t$ steps we have $x_t = (1 - \\eta \\lambda)^t x_0$. This shows how the rate of convergence improves initially as we increase the learning rate $\\eta$ until $\\eta \\lambda = 1$. Beyond that things diverge and for $\\eta \\lambda > 2$ the optimization problem diverges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [0.1, 1, 10, 19]\n",
    "eta = 0.1\n",
    "set_figsize((6, 4))\n",
    "for lam in lambdas:\n",
    "    t = torch.arange(20).detach().numpy()\n",
    "    plt.plot(t, (1 - eta * lam) ** t, label=f'lambda = {lam:.2f}')\n",
    "plt.xlabel('time')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Analyze the conditions for convergence in case we use momentum instead of SGD. What are the feasible values of  $\\eta \\lambda$ for convergence? Is the range of feasible paramenters larger when compared to $0 < \\eta \\lambda < 2$ for gradient descent?.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
